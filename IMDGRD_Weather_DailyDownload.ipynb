{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9sX+Nb4CcUgBxptaSjgQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajeshworM/Yield_Modelling_Automation/blob/main/IMDGRD_Weather_DailyDownload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> <font size ='5'> **Realtime Districts Daily temperature (Minimum) data download for grid data**"
      ],
      "metadata": {
        "id": "y7xCUASBVidU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ncx9_yA9x23W",
        "outputId": "9495392d-76eb-44f9-b1f2-0a5eb71a4297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already have /content/imdpune_min/TEMP.CTL\n",
            "Parsed CTL: nrows=61, ncols=61, x_start=67.5, y_start=7.5\n",
            "Already have /content/imdpune_min/geoboundaries/geoBoundaries-IND-ADM2-all.zip\n",
            "Using district name field: shapeName\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2352417302.py:117: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"centroid\"] = gdf_admin.geometry.centroid\n",
            "/tmp/ipython-input-2352417302.py:118: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"nearest_r\"] = ((gdf_admin.centroid.y - y_start) / y_step).round().astype(int)\n",
            "/tmp/ipython-input-2352417302.py:119: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"nearest_c\"] = ((gdf_admin.centroid.x - x_start) / x_step).round().astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total districts mapped: 728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  5%|▌         | 15/283 [02:46<49:28, 11.08s/it]\n",
            "\n",
            "  0%|          | 1/283 [00:31<2:28:27, 31.59s/it]\u001b[A\n",
            "  1%|          | 2/283 [00:32<1:04:22, 13.75s/it]\u001b[A\n",
            "  1%|          | 3/283 [00:33<36:42,  7.86s/it]  \u001b[A\n",
            "  1%|▏         | 4/283 [00:34<23:43,  5.10s/it]\u001b[A\n",
            "  2%|▏         | 5/283 [00:35<16:43,  3.61s/it]\u001b[A\n",
            "  2%|▏         | 6/283 [00:36<12:20,  2.67s/it]\u001b[A\n",
            "  2%|▏         | 7/283 [00:37<09:44,  2.12s/it]\u001b[A\n",
            "  3%|▎         | 8/283 [00:38<08:07,  1.77s/it]\u001b[A\n",
            "  3%|▎         | 9/283 [00:39<07:04,  1.55s/it]\u001b[A\n",
            "  4%|▎         | 10/283 [00:40<06:15,  1.38s/it]\u001b[A\n",
            "  4%|▍         | 11/283 [00:41<05:42,  1.26s/it]\u001b[A\n",
            "  4%|▍         | 12/283 [00:42<05:13,  1.16s/it]\u001b[A\n",
            "  5%|▍         | 13/283 [00:43<04:49,  1.07s/it]\u001b[A\n",
            "  5%|▍         | 14/283 [00:44<04:38,  1.03s/it]\u001b[A\n",
            "  5%|▌         | 15/283 [00:45<04:26,  1.01it/s]\u001b[A\n",
            "  6%|▌         | 16/283 [00:45<04:14,  1.05it/s]\u001b[A\n",
            "  6%|▌         | 17/283 [00:46<04:05,  1.08it/s]\u001b[A\n",
            "  6%|▋         | 18/283 [00:47<03:59,  1.11it/s]\u001b[A\n",
            "  7%|▋         | 19/283 [00:48<04:04,  1.08it/s]\u001b[A\n",
            "  7%|▋         | 20/283 [00:49<04:03,  1.08it/s]\u001b[A\n",
            "  7%|▋         | 21/283 [00:50<03:56,  1.11it/s]\u001b[A\n",
            "  8%|▊         | 22/283 [00:51<03:53,  1.12it/s]\u001b[A\n",
            "  8%|▊         | 23/283 [00:52<03:51,  1.12it/s]\u001b[A\n",
            "  8%|▊         | 24/283 [00:53<03:55,  1.10it/s]\u001b[A\n",
            "  9%|▉         | 25/283 [00:53<03:51,  1.12it/s]\u001b[A\n",
            "  9%|▉         | 26/283 [00:54<03:47,  1.13it/s]\u001b[A\n",
            " 10%|▉         | 27/283 [00:55<03:45,  1.14it/s]\u001b[A\n",
            " 10%|▉         | 28/283 [00:56<03:49,  1.11it/s]\u001b[A\n",
            " 10%|█         | 29/283 [00:57<03:48,  1.11it/s]\u001b[A\n",
            " 11%|█         | 30/283 [00:58<03:44,  1.13it/s]\u001b[A\n",
            " 11%|█         | 31/283 [00:59<03:42,  1.13it/s]\u001b[A\n",
            " 11%|█▏        | 32/283 [01:00<03:39,  1.14it/s]\u001b[A\n",
            " 12%|█▏        | 33/283 [01:01<03:38,  1.14it/s]\u001b[A\n",
            " 12%|█▏        | 34/283 [01:01<03:36,  1.15it/s]\u001b[A\n",
            " 12%|█▏        | 35/283 [01:33<41:16,  9.99s/it]\u001b[A\n",
            " 13%|█▎        | 36/283 [02:04<1:07:23, 16.37s/it]\u001b[A\n",
            " 13%|█▎        | 37/283 [02:35<1:25:48, 20.93s/it]\u001b[A\n",
            " 13%|█▎        | 38/283 [03:06<1:37:38, 23.91s/it]\u001b[A\n",
            " 14%|█▍        | 39/283 [03:38<1:46:11, 26.11s/it]\u001b[A\n",
            " 14%|█▍        | 40/283 [04:09<1:51:37, 27.56s/it]\u001b[A\n",
            " 14%|█▍        | 41/283 [04:40<1:55:39, 28.68s/it]\u001b[A\n",
            " 15%|█▍        | 42/283 [04:41<1:22:04, 20.43s/it]\u001b[A\n",
            " 15%|█▌        | 43/283 [04:42<58:15, 14.57s/it]  \u001b[A\n",
            " 16%|█▌        | 44/283 [04:43<41:39, 10.46s/it]\u001b[A\n",
            " 16%|█▌        | 45/283 [04:44<30:04,  7.58s/it]\u001b[A\n",
            " 16%|█▋        | 46/283 [04:44<21:58,  5.56s/it]\u001b[A\n",
            " 17%|█▋        | 47/283 [04:45<16:20,  4.16s/it]\u001b[A\n",
            " 17%|█▋        | 48/283 [04:46<12:24,  3.17s/it]\u001b[A\n",
            " 17%|█▋        | 49/283 [04:47<09:39,  2.48s/it]\u001b[A\n",
            " 18%|█▊        | 50/283 [04:48<07:44,  1.99s/it]\u001b[A\n",
            " 18%|█▊        | 51/283 [04:49<06:26,  1.66s/it]\u001b[A\n",
            " 18%|█▊        | 52/283 [04:50<05:28,  1.42s/it]\u001b[A\n",
            " 19%|█▊        | 53/283 [04:51<04:48,  1.25s/it]\u001b[A\n",
            " 19%|█▉        | 54/283 [04:51<04:21,  1.14s/it]\u001b[A\n",
            " 19%|█▉        | 55/283 [04:52<04:00,  1.05s/it]\u001b[A\n",
            " 20%|█▉        | 56/283 [04:53<03:47,  1.00s/it]\u001b[A\n",
            " 20%|██        | 57/283 [04:54<03:37,  1.04it/s]\u001b[A\n",
            " 20%|██        | 58/283 [04:55<03:29,  1.07it/s]\u001b[A\n",
            " 21%|██        | 59/283 [04:56<03:24,  1.10it/s]\u001b[A\n",
            " 21%|██        | 60/283 [04:57<03:20,  1.11it/s]\u001b[A\n",
            " 22%|██▏       | 61/283 [04:58<03:17,  1.12it/s]\u001b[A\n",
            " 22%|██▏       | 62/283 [04:58<03:16,  1.13it/s]\u001b[A\n",
            " 22%|██▏       | 63/283 [04:59<03:13,  1.14it/s]\u001b[A\n",
            " 23%|██▎       | 64/283 [05:00<03:11,  1.14it/s]\u001b[A\n",
            " 23%|██▎       | 65/283 [05:01<03:10,  1.14it/s]\u001b[A\n",
            " 23%|██▎       | 66/283 [05:02<03:08,  1.15it/s]\u001b[A\n",
            " 24%|██▎       | 67/283 [05:03<03:07,  1.15it/s]\u001b[A\n",
            " 24%|██▍       | 68/283 [05:04<03:05,  1.16it/s]\u001b[A\n",
            " 24%|██▍       | 69/283 [05:04<03:04,  1.16it/s]\u001b[A\n",
            " 25%|██▍       | 70/283 [05:05<03:03,  1.16it/s]\u001b[A\n",
            " 25%|██▌       | 71/283 [05:06<03:04,  1.15it/s]\u001b[A\n",
            " 25%|██▌       | 72/283 [05:07<03:03,  1.15it/s]\u001b[A\n",
            " 26%|██▌       | 73/283 [05:08<03:02,  1.15it/s]\u001b[A\n",
            " 26%|██▌       | 74/283 [05:09<03:01,  1.15it/s]\u001b[A\n",
            " 27%|██▋       | 75/283 [05:10<03:01,  1.15it/s]\u001b[A\n",
            " 27%|██▋       | 76/283 [05:11<03:01,  1.14it/s]\u001b[A\n",
            " 27%|██▋       | 77/283 [05:41<33:58,  9.90s/it]\u001b[A\n",
            " 28%|██▊       | 78/283 [06:13<55:28, 16.24s/it]\u001b[A\n",
            " 28%|██▊       | 79/283 [06:43<1:10:09, 20.64s/it]\u001b[A\n",
            " 28%|██▊       | 80/283 [07:14<1:20:14, 23.71s/it]\u001b[A\n",
            " 29%|██▊       | 81/283 [07:45<1:27:05, 25.87s/it]\u001b[A\n",
            " 29%|██▉       | 82/283 [08:17<1:32:55, 27.74s/it]\u001b[A\n",
            " 29%|██▉       | 83/283 [08:49<1:36:00, 28.80s/it]\u001b[A\n",
            " 30%|██▉       | 84/283 [08:50<1:08:03, 20.52s/it]\u001b[A\n",
            " 30%|███       | 85/283 [08:51<48:15, 14.62s/it]  \u001b[A\n",
            " 30%|███       | 86/283 [08:52<34:27, 10.49s/it]\u001b[A\n",
            " 31%|███       | 87/283 [08:52<24:56,  7.63s/it]\u001b[A\n",
            " 31%|███       | 88/283 [08:53<18:13,  5.61s/it]\u001b[A\n",
            " 31%|███▏      | 89/283 [08:54<13:32,  4.19s/it]\u001b[A\n",
            " 32%|███▏      | 90/283 [08:55<10:16,  3.19s/it]\u001b[A\n",
            " 32%|███▏      | 91/283 [08:56<08:07,  2.54s/it]\u001b[A\n",
            " 33%|███▎      | 92/283 [08:57<06:29,  2.04s/it]\u001b[A\n",
            " 33%|███▎      | 93/283 [08:58<05:21,  1.69s/it]\u001b[A\n",
            " 33%|███▎      | 94/283 [08:59<04:33,  1.45s/it]\u001b[A\n",
            " 34%|███▎      | 95/283 [09:00<04:01,  1.29s/it]\u001b[A\n",
            " 34%|███▍      | 96/283 [09:01<03:40,  1.18s/it]\u001b[A\n",
            " 34%|███▍      | 97/283 [09:01<03:21,  1.09s/it]\u001b[A\n",
            " 35%|███▍      | 98/283 [09:02<03:08,  1.02s/it]\u001b[A\n",
            " 35%|███▍      | 99/283 [09:03<03:00,  1.02it/s]\u001b[A\n",
            " 35%|███▌      | 100/283 [09:04<02:59,  1.02it/s]\u001b[A\n",
            " 36%|███▌      | 101/283 [09:05<02:52,  1.05it/s]\u001b[A\n",
            " 36%|███▌      | 102/283 [09:06<02:47,  1.08it/s]\u001b[A\n",
            " 36%|███▋      | 103/283 [09:07<02:45,  1.09it/s]\u001b[A\n",
            " 37%|███▋      | 104/283 [09:08<02:44,  1.09it/s]\u001b[A\n",
            " 37%|███▋      | 105/283 [09:09<02:42,  1.09it/s]\u001b[A\n",
            " 37%|███▋      | 106/283 [09:10<02:39,  1.11it/s]\u001b[A\n",
            " 38%|███▊      | 107/283 [09:10<02:36,  1.13it/s]\u001b[A\n",
            " 38%|███▊      | 108/283 [09:11<02:36,  1.12it/s]\u001b[A\n",
            " 39%|███▊      | 109/283 [09:12<02:39,  1.09it/s]\u001b[A\n",
            " 39%|███▉      | 110/283 [09:13<02:36,  1.10it/s]\u001b[A\n",
            " 39%|███▉      | 111/283 [09:14<02:37,  1.09it/s]\u001b[A\n",
            " 40%|███▉      | 112/283 [09:15<02:34,  1.11it/s]\u001b[A\n",
            " 40%|███▉      | 113/283 [09:16<02:34,  1.10it/s]\u001b[A\n",
            " 40%|████      | 114/283 [09:17<02:32,  1.11it/s]\u001b[A\n",
            " 41%|████      | 115/283 [09:18<02:36,  1.07it/s]\u001b[A\n",
            " 41%|████      | 116/283 [09:19<02:39,  1.05it/s]\u001b[A\n",
            " 41%|████▏     | 117/283 [09:20<02:33,  1.08it/s]\u001b[A\n",
            " 42%|████▏     | 118/283 [09:51<27:17,  9.92s/it]\u001b[A\n",
            " 42%|████▏     | 119/283 [10:21<44:21, 16.23s/it]\u001b[A\n",
            " 42%|████▏     | 120/283 [10:53<56:56, 20.96s/it]\u001b[A\n",
            " 43%|████▎     | 121/283 [11:24<1:04:38, 23.94s/it]\u001b[A\n",
            " 43%|████▎     | 122/283 [11:55<1:09:50, 26.03s/it]\u001b[A\n",
            " 43%|████▎     | 123/283 [12:26<1:13:21, 27.51s/it]\u001b[A\n",
            " 44%|████▍     | 124/283 [12:27<52:00, 19.63s/it]  \u001b[A\n",
            " 44%|████▍     | 125/283 [12:28<36:54, 14.02s/it]\u001b[A\n",
            " 45%|████▍     | 126/283 [12:29<26:24, 10.09s/it]\u001b[A\n",
            " 45%|████▍     | 127/283 [12:30<19:07,  7.35s/it]\u001b[A\n",
            " 45%|████▌     | 128/283 [12:31<13:58,  5.41s/it]\u001b[A\n",
            " 46%|████▌     | 129/283 [12:32<10:29,  4.08s/it]\u001b[A\n",
            " 46%|████▌     | 130/283 [12:33<07:57,  3.12s/it]\u001b[A\n",
            " 46%|████▋     | 131/283 [12:34<06:14,  2.46s/it]\u001b[A\n",
            " 47%|████▋     | 132/283 [12:35<05:01,  1.99s/it]\u001b[A\n",
            " 47%|████▋     | 133/283 [12:36<04:10,  1.67s/it]\u001b[A\n",
            " 47%|████▋     | 134/283 [12:37<03:36,  1.46s/it]\u001b[A\n",
            " 48%|████▊     | 135/283 [12:38<03:09,  1.28s/it]\u001b[A\n",
            " 48%|████▊     | 136/283 [12:38<02:49,  1.16s/it]\u001b[A\n",
            " 48%|████▊     | 137/283 [12:39<02:35,  1.07s/it]\u001b[A\n",
            " 49%|████▉     | 138/283 [12:40<02:26,  1.01s/it]\u001b[A\n",
            " 49%|████▉     | 139/283 [12:41<02:19,  1.03it/s]\u001b[A\n",
            " 49%|████▉     | 140/283 [12:42<02:14,  1.06it/s]\u001b[A\n",
            " 50%|████▉     | 141/283 [12:43<02:10,  1.09it/s]\u001b[A\n",
            " 50%|█████     | 142/283 [12:44<02:08,  1.09it/s]\u001b[A\n",
            " 51%|█████     | 143/283 [12:45<02:08,  1.09it/s]\u001b[A\n",
            " 51%|█████     | 144/283 [12:46<02:05,  1.11it/s]\u001b[A\n",
            " 51%|█████     | 145/283 [12:46<02:05,  1.10it/s]\u001b[A\n",
            " 52%|█████▏    | 146/283 [12:47<02:03,  1.11it/s]\u001b[A\n",
            " 52%|█████▏    | 147/283 [12:48<02:06,  1.08it/s]\u001b[A\n",
            " 52%|█████▏    | 148/283 [12:49<02:02,  1.10it/s]\u001b[A\n",
            " 53%|█████▎    | 149/283 [12:50<02:01,  1.11it/s]\u001b[A\n",
            " 53%|█████▎    | 150/283 [12:51<02:00,  1.11it/s]\u001b[A\n",
            " 53%|█████▎    | 151/283 [12:52<02:02,  1.07it/s]\u001b[A\n",
            " 54%|█████▎    | 152/283 [12:53<02:00,  1.08it/s]\u001b[A\n",
            " 54%|█████▍    | 153/283 [12:54<01:58,  1.10it/s]\u001b[A\n",
            " 54%|█████▍    | 154/283 [12:55<01:56,  1.11it/s]\u001b[A\n",
            " 55%|█████▍    | 155/283 [12:56<01:54,  1.11it/s]\u001b[A\n",
            " 55%|█████▌    | 156/283 [12:56<01:53,  1.12it/s]\u001b[A\n",
            " 55%|█████▌    | 157/283 [12:57<01:52,  1.12it/s]\u001b[A\n",
            " 56%|█████▌    | 158/283 [13:28<20:38,  9.91s/it]\u001b[A\n",
            " 56%|█████▌    | 159/283 [13:59<33:28, 16.20s/it]\u001b[A\n",
            " 57%|█████▋    | 160/283 [14:00<24:04, 11.74s/it]\u001b[A\n",
            " 57%|█████▋    | 161/283 [14:01<17:14,  8.48s/it]\u001b[A\n",
            " 57%|█████▋    | 162/283 [14:02<12:29,  6.19s/it]\u001b[A\n",
            " 58%|█████▊    | 163/283 [14:03<09:11,  4.59s/it]\u001b[A\n",
            " 58%|█████▊    | 164/283 [14:04<06:55,  3.49s/it]\u001b[A\n",
            " 58%|█████▊    | 165/283 [14:05<05:19,  2.71s/it]\u001b[A\n",
            " 59%|█████▊    | 166/283 [14:06<04:11,  2.15s/it]\u001b[A\n",
            " 59%|█████▉    | 167/283 [14:07<03:25,  1.77s/it]\u001b[A\n",
            " 59%|█████▉    | 168/283 [14:07<02:53,  1.51s/it]\u001b[A\n",
            " 60%|█████▉    | 169/283 [14:08<02:31,  1.33s/it]\u001b[A\n",
            " 60%|██████    | 170/283 [14:09<02:15,  1.20s/it]\u001b[A\n",
            " 60%|██████    | 171/283 [14:10<02:03,  1.10s/it]\u001b[A\n",
            " 61%|██████    | 172/283 [14:11<01:54,  1.03s/it]\u001b[A\n",
            " 61%|██████    | 173/283 [14:12<01:49,  1.00it/s]\u001b[A\n",
            " 61%|██████▏   | 174/283 [14:13<01:44,  1.04it/s]\u001b[A\n",
            " 62%|██████▏   | 175/283 [14:14<01:40,  1.08it/s]\u001b[A\n",
            " 62%|██████▏   | 176/283 [14:15<01:37,  1.10it/s]\u001b[A\n",
            " 63%|██████▎   | 177/283 [14:15<01:37,  1.08it/s]\u001b[A\n",
            " 63%|██████▎   | 178/283 [14:16<01:36,  1.09it/s]\u001b[A\n",
            " 63%|██████▎   | 179/283 [14:17<01:33,  1.11it/s]\u001b[A\n",
            " 64%|██████▎   | 180/283 [14:18<01:32,  1.12it/s]\u001b[A\n",
            " 64%|██████▍   | 181/283 [14:19<01:30,  1.13it/s]\u001b[A\n",
            " 64%|██████▍   | 182/283 [14:20<01:29,  1.13it/s]\u001b[A\n",
            " 65%|██████▍   | 183/283 [14:21<01:27,  1.14it/s]\u001b[A\n",
            " 65%|██████▌   | 184/283 [14:22<01:26,  1.14it/s]\u001b[A\n",
            " 65%|██████▌   | 185/283 [14:22<01:25,  1.15it/s]\u001b[A\n",
            " 66%|██████▌   | 186/283 [14:23<01:25,  1.14it/s]\u001b[A\n",
            " 66%|██████▌   | 187/283 [14:24<01:26,  1.11it/s]\u001b[A\n",
            " 66%|██████▋   | 188/283 [14:25<01:24,  1.13it/s]\u001b[A\n",
            " 67%|██████▋   | 189/283 [14:26<01:22,  1.13it/s]\u001b[A\n",
            " 67%|██████▋   | 190/283 [14:27<01:22,  1.13it/s]\u001b[A\n",
            " 67%|██████▋   | 191/283 [14:28<01:24,  1.09it/s]\u001b[A\n",
            " 68%|██████▊   | 192/283 [14:29<01:21,  1.11it/s]\u001b[A\n",
            " 68%|██████▊   | 193/283 [14:30<01:20,  1.12it/s]\u001b[A\n",
            " 69%|██████▊   | 194/283 [15:01<14:49,  9.99s/it]\u001b[A\n",
            " 69%|██████▉   | 195/283 [15:33<24:10, 16.48s/it]\u001b[A\n",
            " 69%|██████▉   | 196/283 [16:04<30:14, 20.86s/it]\u001b[A\n",
            " 70%|██████▉   | 197/283 [16:35<34:16, 23.91s/it]\u001b[A\n",
            " 70%|██████▉   | 198/283 [17:06<36:54, 26.05s/it]\u001b[A\n",
            " 70%|███████   | 199/283 [17:37<38:31, 27.52s/it]\u001b[A\n",
            " 71%|███████   | 200/283 [17:38<27:01, 19.53s/it]\u001b[A\n",
            " 71%|███████   | 201/283 [18:08<31:21, 22.94s/it]\u001b[A\n",
            " 71%|███████▏  | 202/283 [18:09<22:01, 16.32s/it]\u001b[A\n",
            " 72%|███████▏  | 203/283 [18:40<27:35, 20.69s/it]\u001b[A\n",
            " 72%|███████▏  | 204/283 [19:11<31:16, 23.75s/it]\u001b[A\n",
            " 72%|███████▏  | 205/283 [19:12<21:57, 16.89s/it]\u001b[A\n",
            " 73%|███████▎  | 206/283 [19:13<15:45, 12.28s/it]\u001b[A\n",
            " 73%|███████▎  | 207/283 [19:14<11:12,  8.85s/it]\u001b[A\n",
            " 73%|███████▎  | 208/283 [19:15<08:04,  6.46s/it]\u001b[A\n",
            " 74%|███████▍  | 209/283 [19:16<05:55,  4.80s/it]\u001b[A\n",
            " 74%|███████▍  | 210/283 [19:17<04:24,  3.62s/it]\u001b[A\n",
            " 75%|███████▍  | 211/283 [19:18<03:21,  2.79s/it]\u001b[A\n",
            " 75%|███████▍  | 212/283 [19:19<02:37,  2.21s/it]\u001b[A\n",
            " 75%|███████▌  | 213/283 [19:20<02:06,  1.81s/it]\u001b[A\n",
            " 76%|███████▌  | 214/283 [19:20<01:45,  1.54s/it]\u001b[A\n",
            " 76%|███████▌  | 215/283 [19:21<01:30,  1.33s/it]\u001b[A\n",
            " 76%|███████▋  | 216/283 [19:22<01:20,  1.20s/it]\u001b[A\n",
            " 77%|███████▋  | 217/283 [19:23<01:12,  1.10s/it]\u001b[A\n",
            " 77%|███████▋  | 218/283 [19:24<01:08,  1.05s/it]\u001b[A\n",
            " 77%|███████▋  | 219/283 [19:25<01:05,  1.02s/it]\u001b[A\n",
            " 78%|███████▊  | 220/283 [19:26<01:01,  1.03it/s]\u001b[A\n",
            " 78%|███████▊  | 221/283 [19:27<00:58,  1.06it/s]\u001b[A\n",
            " 78%|███████▊  | 222/283 [19:28<00:56,  1.09it/s]\u001b[A\n",
            " 79%|███████▉  | 223/283 [19:28<00:54,  1.10it/s]\u001b[A\n",
            " 79%|███████▉  | 224/283 [19:29<00:52,  1.12it/s]\u001b[A\n",
            " 80%|███████▉  | 225/283 [19:30<00:51,  1.13it/s]\u001b[A\n",
            " 80%|███████▉  | 226/283 [19:31<00:50,  1.14it/s]\u001b[A\n",
            " 80%|████████  | 227/283 [19:32<00:48,  1.15it/s]\u001b[A\n",
            " 81%|████████  | 228/283 [19:33<00:48,  1.14it/s]\u001b[A\n",
            " 81%|████████  | 229/283 [19:34<00:47,  1.14it/s]\u001b[A\n",
            " 81%|████████▏ | 230/283 [19:35<00:46,  1.15it/s]\u001b[A\n",
            " 82%|████████▏ | 231/283 [19:35<00:45,  1.15it/s]\u001b[A\n",
            " 82%|████████▏ | 232/283 [19:36<00:44,  1.15it/s]\u001b[A\n",
            " 82%|████████▏ | 233/283 [19:37<00:43,  1.14it/s]\u001b[A\n",
            " 83%|████████▎ | 234/283 [19:38<00:43,  1.13it/s]\u001b[A\n",
            " 83%|████████▎ | 235/283 [19:39<00:42,  1.13it/s]\u001b[A\n",
            " 83%|████████▎ | 236/283 [19:40<00:41,  1.14it/s]\u001b[A\n",
            " 84%|████████▎ | 237/283 [19:41<00:40,  1.14it/s]\u001b[A\n",
            " 84%|████████▍ | 238/283 [19:42<00:40,  1.11it/s]\u001b[A\n",
            " 84%|████████▍ | 239/283 [19:43<00:39,  1.12it/s]\u001b[A\n",
            " 85%|████████▍ | 240/283 [20:14<07:10, 10.01s/it]\u001b[A\n",
            " 85%|████████▌ | 241/283 [20:45<11:28, 16.39s/it]\u001b[A\n",
            " 86%|████████▌ | 242/283 [21:16<14:10, 20.73s/it]\u001b[A\n",
            " 86%|████████▌ | 243/283 [21:17<09:56, 14.90s/it]\u001b[A\n",
            " 86%|████████▌ | 244/283 [21:18<06:57, 10.70s/it]\u001b[A\n",
            " 87%|████████▋ | 245/283 [21:19<04:54,  7.75s/it]\u001b[A\n",
            " 87%|████████▋ | 246/283 [21:20<03:30,  5.68s/it]\u001b[A\n",
            " 87%|████████▋ | 247/283 [21:21<02:32,  4.24s/it]\u001b[A\n",
            " 88%|████████▊ | 248/283 [21:22<01:52,  3.22s/it]\u001b[A\n",
            " 88%|████████▊ | 249/283 [21:22<01:25,  2.52s/it]\u001b[A\n",
            " 88%|████████▊ | 250/283 [21:23<01:06,  2.02s/it]\u001b[A\n",
            " 89%|████████▊ | 251/283 [21:24<00:53,  1.67s/it]\u001b[A\n",
            " 89%|████████▉ | 252/283 [21:25<00:44,  1.43s/it]\u001b[A\n",
            " 89%|████████▉ | 253/283 [21:26<00:37,  1.26s/it]\u001b[A\n",
            " 90%|████████▉ | 254/283 [21:27<00:33,  1.15s/it]\u001b[A\n",
            " 90%|█████████ | 255/283 [21:28<00:30,  1.08s/it]\u001b[A\n",
            " 90%|█████████ | 256/283 [21:29<00:27,  1.02s/it]\u001b[A\n",
            " 91%|█████████ | 257/283 [21:29<00:25,  1.03it/s]\u001b[A\n",
            " 91%|█████████ | 258/283 [21:30<00:23,  1.06it/s]\u001b[A\n",
            " 92%|█████████▏| 259/283 [21:31<00:22,  1.08it/s]\u001b[A\n",
            " 92%|█████████▏| 260/283 [21:32<00:20,  1.10it/s]\u001b[A\n",
            " 92%|█████████▏| 261/283 [21:33<00:19,  1.12it/s]\u001b[A\n",
            " 93%|█████████▎| 262/283 [21:34<00:18,  1.13it/s]\u001b[A\n",
            " 93%|█████████▎| 263/283 [21:35<00:17,  1.13it/s]\u001b[A\n",
            " 93%|█████████▎| 264/283 [21:36<00:16,  1.12it/s]\u001b[A\n",
            " 94%|█████████▎| 265/283 [21:36<00:16,  1.12it/s]\u001b[A\n",
            " 94%|█████████▍| 266/283 [21:37<00:15,  1.12it/s]\u001b[A\n",
            " 94%|█████████▍| 267/283 [21:38<00:14,  1.10it/s]\u001b[A\n",
            " 95%|█████████▍| 268/283 [21:39<00:13,  1.12it/s]\u001b[A\n",
            " 95%|█████████▌| 269/283 [21:40<00:12,  1.13it/s]\u001b[A\n",
            " 95%|█████████▌| 270/283 [21:41<00:11,  1.14it/s]\u001b[A\n",
            " 96%|█████████▌| 271/283 [21:42<00:10,  1.14it/s]\u001b[A\n",
            " 96%|█████████▌| 272/283 [21:43<00:09,  1.15it/s]\u001b[A\n",
            " 96%|█████████▋| 273/283 [21:44<00:08,  1.13it/s]\u001b[A\n",
            " 97%|█████████▋| 274/283 [21:44<00:08,  1.11it/s]\u001b[A\n",
            " 97%|█████████▋| 275/283 [21:45<00:07,  1.12it/s]\u001b[A\n",
            " 98%|█████████▊| 276/283 [21:46<00:06,  1.13it/s]\u001b[A\n",
            " 98%|█████████▊| 277/283 [22:17<00:59,  9.90s/it]\u001b[A\n",
            " 98%|█████████▊| 278/283 [22:48<01:20, 16.19s/it]\u001b[A\n",
            " 99%|█████████▊| 279/283 [23:19<01:22, 20.70s/it]\u001b[A\n",
            " 99%|█████████▉| 280/283 [23:20<00:44, 14.75s/it]\u001b[A\n",
            " 99%|█████████▉| 281/283 [23:21<00:21, 10.70s/it]\u001b[A\n",
            "100%|█████████▉| 282/283 [23:22<00:07,  7.75s/it]\u001b[A\n",
            "100%|██████████| 283/283 [23:23<00:00,  4.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded GRD files: 283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 283/283 [00:00<00:00, 1069.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved combined CSV: /content/imdpune_min/district_daily_min_temp_2025.csv\n",
            "CSV shape: (283, 729)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f532946-5a60-4495-85f4-a28849384172\", \"district_daily_min_temp_2025.csv\", 3840530)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Colab-ready: download IMD .grd daily files, map grid cells -> all districts, produce district-wise CSV\n",
        "# Run this in Google Colab. Adjust start_date / end_date and paths as needed.\n",
        "\n",
        "# 0) Install dependencies (may take a minute)\n",
        "!pip install -q geopandas requests tqdm shapely pyproj fiona\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "IMD_BASE = \"https://www.imdpune.gov.in/cmpg/Realtimedata/min/\"\n",
        "CTL_URL  = IMD_BASE + \"TEMP.CTL\"\n",
        "GRD_FILENAME_PATTERN = \"min{dd}{mm}{yyyy}.grd\"\n",
        "\n",
        "GEOBOUNDARIES_IND_ADM2_ZIP = \"https://github.com/wmgeolab/geoBoundaries/raw/9469f09/releaseData/gbOpen/IND/ADM2/geoBoundaries-IND-ADM2-all.zip\"\n",
        "\n",
        "WORKDIR = \"/content/imdpune_min\"\n",
        "GRD_DIR = os.path.join(WORKDIR, \"grds\")\n",
        "GEO_DIR = os.path.join(WORKDIR, \"geoboundaries\")\n",
        "OUT_CSV = os.path.join(WORKDIR, \"district_daily_min_temp_2025.csv\")\n",
        "\n",
        "os.makedirs(WORKDIR, exist_ok=True)\n",
        "os.makedirs(GRD_DIR, exist_ok=True)\n",
        "os.makedirs(GEO_DIR, exist_ok=True)\n",
        "\n",
        "start_date = datetime(2025, 1, 1)\n",
        "end_date   = datetime.today()\n",
        "\n",
        "small_test = False\n",
        "if small_test:\n",
        "    end_date = start_date + timedelta(days=2)\n",
        "\n",
        "# ------------------ UTILITY ------------------\n",
        "def download_file(url, out_path, stream=True, overwrite=False):\n",
        "    if os.path.exists(out_path) and not overwrite:\n",
        "        print(\"Already have\", out_path)\n",
        "        return out_path\n",
        "    print(\"Downloading:\", url)\n",
        "    r = requests.get(url, stream=stream, timeout=60)\n",
        "    if r.status_code != 200:\n",
        "        print(f\"Failed to download {url} (status {r.status_code})\")\n",
        "        return None\n",
        "    with open(out_path, \"wb\") as fh:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                fh.write(chunk)\n",
        "    print(\"Saved ->\", out_path)\n",
        "    return out_path\n",
        "\n",
        "# ------------------ 1) Download and parse CTL ------------------\n",
        "ctl_local = os.path.join(WORKDIR, \"TEMP.CTL\")\n",
        "download_file(CTL_URL, ctl_local)\n",
        "\n",
        "meta = {}\n",
        "with open(ctl_local, \"rt\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if not parts: continue\n",
        "        key = parts[0].upper()\n",
        "        if key == \"XDEF\":\n",
        "            meta[\"ncols\"] = int(parts[1])\n",
        "            meta[\"x_start\"] = float(parts[3])\n",
        "            meta[\"x_step\"]  = float(parts[4])\n",
        "        elif key == \"YDEF\":\n",
        "            meta[\"nrows\"] = int(parts[1])\n",
        "            meta[\"y_start\"] = float(parts[3])\n",
        "            meta[\"y_step\"]  = float(parts[4])\n",
        "        elif key == \"UNDEF\":\n",
        "            meta[\"undef\"] = float(parts[1])\n",
        "\n",
        "ncols, nrows = meta[\"ncols\"], meta[\"nrows\"]\n",
        "x_start, y_start = meta[\"x_start\"], meta[\"y_start\"]\n",
        "x_step, y_step = meta[\"x_step\"], meta[\"y_step\"]\n",
        "nodata_value = meta[\"undef\"]\n",
        "\n",
        "print(f\"Parsed CTL: nrows={nrows}, ncols={ncols}, x_start={x_start}, y_start={y_start}\")\n",
        "\n",
        "# ------------------ 2) Download India district boundaries ------------------\n",
        "geo_zip_local = os.path.join(GEO_DIR, \"geoBoundaries-IND-ADM2-all.zip\")\n",
        "download_file(GEOBOUNDARIES_IND_ADM2_ZIP, geo_zip_local)\n",
        "\n",
        "with zipfile.ZipFile(geo_zip_local, \"r\") as z:\n",
        "    z.extractall(GEO_DIR)\n",
        "    members = z.namelist()\n",
        "\n",
        "found_geojson = next((os.path.join(GEO_DIR,m) for m in members if m.lower().endswith(\".geojson\")), None)\n",
        "found_shp = next((os.path.join(GEO_DIR,m) for m in members if m.lower().endswith(\".shp\") and \"ADM2\" in m), None)\n",
        "\n",
        "if found_geojson:\n",
        "    gdf_admin = gpd.read_file(found_geojson)\n",
        "elif found_shp:\n",
        "    gdf_admin = gpd.read_file(found_shp)\n",
        "else:\n",
        "    cand = [os.path.join(GEO_DIR, f) for f in members if f.lower().endswith((\".shp\",\".geojson\"))]\n",
        "    if not cand: raise RuntimeError(\"No shapefile/geojson found.\")\n",
        "    gdf_admin = gpd.read_file(cand[0])\n",
        "\n",
        "if gdf_admin.crs is None or gdf_admin.crs.to_string() != \"EPSG:4326\":\n",
        "    gdf_admin = gdf_admin.to_crs(epsg=4326)\n",
        "\n",
        "possible_name_fields = [\"shapeName\",\"shapeName_en\",\"NAME_2\",\"NAME_1\",\"NAME\"]\n",
        "district_name_field = next((f for f in possible_name_fields if f in gdf_admin.columns), None)\n",
        "if district_name_field is None:\n",
        "    district_name_field = [c for c in gdf_admin.columns if c != gdf_admin.geometry.name][0]\n",
        "\n",
        "print(\"Using district name field:\", district_name_field)\n",
        "\n",
        "# ------------------ 3) Centroid mapping (ensures all districts) ------------------\n",
        "gdf_admin[\"centroid\"] = gdf_admin.geometry.centroid\n",
        "gdf_admin[\"nearest_r\"] = ((gdf_admin.centroid.y - y_start) / y_step).round().astype(int)\n",
        "gdf_admin[\"nearest_c\"] = ((gdf_admin.centroid.x - x_start) / x_step).round().astype(int)\n",
        "\n",
        "# Clip to valid grid\n",
        "gdf_admin[\"nearest_r\"] = gdf_admin[\"nearest_r\"].clip(0, nrows-1)\n",
        "gdf_admin[\"nearest_c\"] = gdf_admin[\"nearest_c\"].clip(0, ncols-1)\n",
        "\n",
        "# Build mapping dict: district_name -> (r,c)\n",
        "district_to_grid = dict(zip(gdf_admin[district_name_field], zip(gdf_admin[\"nearest_r\"], gdf_admin[\"nearest_c\"])))\n",
        "print(\"Total districts mapped:\", len(district_to_grid))\n",
        "\n",
        "# ------------------ 4) Download all .grd files ------------------\n",
        "date = start_date\n",
        "downloaded_files = []\n",
        "pbar = tqdm(total=(end_date - start_date).days + 1)\n",
        "while date <= end_date:\n",
        "    fname = GRD_FILENAME_PATTERN.format(dd=f\"{date.day:02d}\", mm=f\"{date.month:02d}\", yyyy=f\"{date.year}\")\n",
        "    url = IMD_BASE + fname\n",
        "    local_path = os.path.join(GRD_DIR, fname)\n",
        "    r = requests.get(url, stream=True, timeout=30)\n",
        "    if r.status_code == 200:\n",
        "        with open(local_path, \"wb\") as fh:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk: fh.write(chunk)\n",
        "        downloaded_files.append(local_path)\n",
        "    date += timedelta(days=1)\n",
        "    pbar.update(1)\n",
        "pbar.close()\n",
        "print(\"Downloaded GRD files:\", len(downloaded_files))\n",
        "\n",
        "if len(downloaded_files) == 0:\n",
        "    raise RuntimeError(\"No .grd files downloaded.\")\n",
        "\n",
        "# ------------------ 5) Read .grd files ------------------\n",
        "def read_grid_best_guess(path):\n",
        "    try:\n",
        "        arr = np.fromfile(path, dtype=\"<f4\").reshape((nrows,ncols))\n",
        "    except:\n",
        "        arr = np.fromfile(path, dtype=\">f4\").reshape((nrows,ncols))\n",
        "    return arr\n",
        "\n",
        "# ------------------ 6) Aggregate to districts using centroids ------------------\n",
        "records = []\n",
        "for local_path in tqdm(sorted(downloaded_files)):\n",
        "    fname = os.path.basename(local_path)\n",
        "    dd, mm, yyyy = int(fname[3:5]), int(fname[5:7]), int(fname[7:11])\n",
        "    dt_str = datetime(yyyy, mm, dd).strftime(\"%Y-%m-%d\")\n",
        "    arr = read_grid_best_guess(local_path)\n",
        "    rec = {\"date\": dt_str}\n",
        "    for district, (r,c) in district_to_grid.items():\n",
        "        val = arr[r,c]\n",
        "        rec[district] = float(np.nan) if val == nodata_value else float(val)\n",
        "    records.append(rec)\n",
        "\n",
        "df = pd.DataFrame(records)\n",
        "cols = ['date'] + [c for c in df.columns if c != 'date']\n",
        "df = df[cols].sort_values(\"date\").reset_index(drop=True)\n",
        "df.to_csv(OUT_CSV, index=False)\n",
        "print(\"\\nSaved combined CSV:\", OUT_CSV)\n",
        "print(\"CSV shape:\", df.shape)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(OUT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> <font size ='5'> **Realtime Districts Daily temperature (Maximum) data download for grid data**"
      ],
      "metadata": {
        "id": "4MuPm4AAWRsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DSET p:\\rain_ind0.25_19_03_01.GRD\n",
        "TITLE 0.25 DEGREE ANALYZED GRIDS\n",
        "UNDEF  -999.0\n",
        "XDEF  135  LINEAR 66.5 0.25\n",
        "YDEF  129  LINEAR 6.5 0.25\n",
        "ZDEF    1  LINEAR 1 1\n",
        "TDEF   1  LINEAR JUN2006 1DY\n",
        "VARS  1\n",
        "RF 0 99 RFANOMALY\n",
        "ENDVARS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wL2oLL7SQ2-y",
        "outputId": "3bdd974c-80e9-4771-8a46-aa8fe19fe1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: https://www.imdpune.gov.in/cmpg/Realtimedata/max/TEMP.CTL\n",
            "Saved -> /content/imdpune_max/TEMP.CTL\n",
            "Parsed CTL: nrows=61, ncols=61, x_start=67.5, y_start=7.5\n",
            "Downloading: https://github.com/wmgeolab/geoBoundaries/raw/9469f09/releaseData/gbOpen/IND/ADM2/geoBoundaries-IND-ADM2-all.zip\n",
            "Saved -> /content/imdpune_max/geoboundaries/geoBoundaries-IND-ADM2-all.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-829717817.py:117: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"centroid\"] = gdf_admin.geometry.centroid\n",
            "/tmp/ipython-input-829717817.py:118: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"nearest_r\"] = ((gdf_admin.centroid.y - y_start) / y_step).round().astype(int)\n",
            "/tmp/ipython-input-829717817.py:119: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
            "\n",
            "  gdf_admin[\"nearest_c\"] = ((gdf_admin.centroid.x - x_start) / x_step).round().astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using district name field: shapeName\n",
            "Total districts mapped: 728\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/283 [02:57<?, ?it/s]\n",
            "\n",
            "  0%|          | 1/283 [00:01<06:54,  1.47s/it]\u001b[A\n",
            "  1%|          | 2/283 [00:02<05:13,  1.11s/it]\u001b[A\n",
            "  1%|          | 3/283 [00:03<04:43,  1.01s/it]\u001b[A\n",
            "  1%|▏         | 4/283 [00:04<04:27,  1.04it/s]\u001b[A\n",
            "  2%|▏         | 5/283 [00:04<04:17,  1.08it/s]\u001b[A\n",
            "  2%|▏         | 6/283 [00:05<04:10,  1.10it/s]\u001b[A\n",
            "  2%|▏         | 7/283 [00:06<04:08,  1.11it/s]\u001b[A\n",
            "  3%|▎         | 8/283 [00:07<04:12,  1.09it/s]\u001b[A\n",
            "  3%|▎         | 9/283 [00:08<04:08,  1.10it/s]\u001b[A\n",
            "  4%|▎         | 10/283 [00:09<04:05,  1.11it/s]\u001b[A\n",
            "  4%|▍         | 11/283 [00:10<04:02,  1.12it/s]\u001b[A\n",
            "  4%|▍         | 12/283 [00:11<03:58,  1.14it/s]\u001b[A\n",
            "  5%|▍         | 13/283 [00:12<03:58,  1.13it/s]\u001b[A\n",
            "  5%|▍         | 14/283 [00:12<03:56,  1.14it/s]\u001b[A\n",
            "  5%|▌         | 15/283 [00:13<03:53,  1.15it/s]\u001b[A\n",
            "  6%|▌         | 16/283 [00:14<03:52,  1.15it/s]\u001b[A\n",
            "  6%|▌         | 17/283 [00:15<03:50,  1.15it/s]\u001b[A\n",
            "  6%|▋         | 18/283 [00:16<03:50,  1.15it/s]\u001b[A\n",
            "  7%|▋         | 19/283 [00:17<03:50,  1.14it/s]\u001b[A\n",
            "  7%|▋         | 20/283 [00:18<03:49,  1.15it/s]\u001b[A\n",
            "  7%|▋         | 21/283 [00:19<03:49,  1.14it/s]\u001b[A\n",
            "  8%|▊         | 22/283 [00:19<03:48,  1.14it/s]\u001b[A\n",
            "  8%|▊         | 23/283 [00:20<03:46,  1.15it/s]\u001b[A\n",
            "  8%|▊         | 24/283 [00:21<03:45,  1.15it/s]\u001b[A\n",
            "  9%|▉         | 25/283 [00:22<03:46,  1.14it/s]\u001b[A\n",
            "  9%|▉         | 26/283 [00:23<03:44,  1.15it/s]\u001b[A\n",
            " 10%|▉         | 27/283 [00:24<03:44,  1.14it/s]\u001b[A\n",
            " 10%|▉         | 28/283 [00:25<03:42,  1.15it/s]\u001b[A\n",
            " 10%|█         | 29/283 [00:25<03:40,  1.15it/s]\u001b[A\n",
            " 11%|█         | 30/283 [00:26<03:40,  1.15it/s]\u001b[A\n",
            " 11%|█         | 31/283 [00:27<03:39,  1.15it/s]\u001b[A\n",
            " 11%|█▏        | 32/283 [00:28<03:37,  1.15it/s]\u001b[A\n",
            " 12%|█▏        | 33/283 [00:29<03:38,  1.14it/s]\u001b[A\n",
            " 12%|█▏        | 34/283 [00:30<03:36,  1.15it/s]\u001b[A\n",
            " 12%|█▏        | 35/283 [01:01<41:19, 10.00s/it]\u001b[A\n",
            " 13%|█▎        | 36/283 [01:03<30:37,  7.44s/it]\u001b[A\n",
            " 13%|█▎        | 37/283 [01:03<22:24,  5.47s/it]\u001b[A\n",
            " 13%|█▎        | 38/283 [01:04<16:42,  4.09s/it]\u001b[A\n",
            " 14%|█▍        | 39/283 [01:05<12:44,  3.13s/it]\u001b[A\n",
            " 14%|█▍        | 40/283 [01:06<09:56,  2.46s/it]\u001b[A\n",
            " 14%|█▍        | 41/283 [01:07<07:58,  1.98s/it]\u001b[A\n",
            " 15%|█▍        | 42/283 [01:08<06:35,  1.64s/it]\u001b[A\n",
            " 15%|█▌        | 43/283 [01:09<05:38,  1.41s/it]\u001b[A\n",
            " 16%|█▌        | 44/283 [01:10<04:59,  1.25s/it]\u001b[A\n",
            " 16%|█▌        | 45/283 [01:10<04:31,  1.14s/it]\u001b[A\n",
            " 16%|█▋        | 46/283 [01:11<04:10,  1.06s/it]\u001b[A\n",
            " 17%|█▋        | 47/283 [01:12<03:55,  1.00it/s]\u001b[A\n",
            " 17%|█▋        | 48/283 [01:13<03:47,  1.03it/s]\u001b[A\n",
            " 17%|█▋        | 49/283 [01:14<03:38,  1.07it/s]\u001b[A\n",
            " 18%|█▊        | 50/283 [01:15<03:33,  1.09it/s]\u001b[A\n",
            " 18%|█▊        | 51/283 [01:16<03:30,  1.10it/s]\u001b[A\n",
            " 18%|█▊        | 52/283 [01:17<03:26,  1.12it/s]\u001b[A\n",
            " 19%|█▊        | 53/283 [01:17<03:23,  1.13it/s]\u001b[A\n",
            " 19%|█▉        | 54/283 [01:18<03:21,  1.14it/s]\u001b[A\n",
            " 19%|█▉        | 55/283 [01:19<03:19,  1.14it/s]\u001b[A\n",
            " 20%|█▉        | 56/283 [01:20<03:17,  1.15it/s]\u001b[A\n",
            " 20%|██        | 57/283 [01:21<03:17,  1.14it/s]\u001b[A\n",
            " 20%|██        | 58/283 [01:22<03:15,  1.15it/s]\u001b[A\n",
            " 21%|██        | 59/283 [01:23<03:14,  1.15it/s]\u001b[A\n",
            " 21%|██        | 60/283 [01:24<03:32,  1.05it/s]\u001b[A\n",
            " 22%|██▏       | 61/283 [01:25<03:38,  1.02it/s]\u001b[A\n",
            " 22%|██▏       | 62/283 [01:26<03:42,  1.01s/it]\u001b[A\n",
            " 22%|██▏       | 63/283 [01:27<03:46,  1.03s/it]\u001b[A\n",
            " 23%|██▎       | 64/283 [01:28<03:46,  1.04s/it]\u001b[A\n",
            " 23%|██▎       | 65/283 [01:29<03:49,  1.05s/it]\u001b[A\n",
            " 23%|██▎       | 66/283 [01:30<03:49,  1.06s/it]\u001b[A\n",
            " 24%|██▎       | 67/283 [01:31<03:54,  1.09s/it]\u001b[A\n",
            " 24%|██▍       | 68/283 [02:03<36:20, 10.14s/it]\u001b[A\n",
            " 24%|██▍       | 69/283 [02:34<58:35, 16.43s/it]\u001b[A\n",
            " 25%|██▍       | 70/283 [03:05<1:14:17, 20.93s/it]\u001b[A\n",
            " 25%|██▌       | 71/283 [03:37<1:25:02, 24.07s/it]\u001b[A\n",
            " 25%|██▌       | 72/283 [04:08<1:32:33, 26.32s/it]\u001b[A\n",
            " 26%|██▌       | 73/283 [04:39<1:36:54, 27.69s/it]\u001b[A\n",
            " 26%|██▌       | 74/283 [04:40<1:08:24, 19.64s/it]\u001b[A\n",
            " 27%|██▋       | 75/283 [05:11<1:20:09, 23.12s/it]\u001b[A\n",
            " 27%|██▋       | 76/283 [05:42<1:28:10, 25.56s/it]\u001b[A\n",
            " 27%|██▋       | 77/283 [06:14<1:33:33, 27.25s/it]\u001b[A\n",
            " 28%|██▊       | 78/283 [06:44<1:36:51, 28.35s/it]\u001b[A\n",
            " 28%|██▊       | 79/283 [07:15<1:39:02, 29.13s/it]\u001b[A\n",
            " 28%|██▊       | 80/283 [07:46<1:40:20, 29.66s/it]\u001b[A\n",
            " 29%|██▊       | 81/283 [08:17<1:41:23, 30.12s/it]\u001b[A\n",
            " 29%|██▉       | 82/283 [08:49<1:42:00, 30.45s/it]\u001b[A\n",
            " 29%|██▉       | 83/283 [08:50<1:12:30, 21.75s/it]\u001b[A\n",
            " 30%|██▉       | 84/283 [08:51<51:22, 15.49s/it]  \u001b[A\n",
            " 30%|███       | 85/283 [08:52<36:37, 11.10s/it]\u001b[A\n",
            " 30%|███       | 86/283 [08:53<26:24,  8.04s/it]\u001b[A\n",
            " 31%|███       | 87/283 [08:54<19:15,  5.90s/it]\u001b[A\n",
            " 31%|███       | 88/283 [08:55<14:15,  4.39s/it]\u001b[A\n",
            " 31%|███▏      | 89/283 [08:55<10:46,  3.33s/it]\u001b[A\n",
            " 32%|███▏      | 90/283 [08:56<08:20,  2.59s/it]\u001b[A\n",
            " 32%|███▏      | 91/283 [08:57<06:38,  2.08s/it]\u001b[A\n",
            " 33%|███▎      | 92/283 [08:58<05:26,  1.71s/it]\u001b[A\n",
            " 33%|███▎      | 93/283 [08:59<04:36,  1.46s/it]\u001b[A\n",
            " 33%|███▎      | 94/283 [09:00<04:02,  1.28s/it]\u001b[A\n",
            " 34%|███▎      | 95/283 [09:01<03:45,  1.20s/it]\u001b[A\n",
            " 34%|███▍      | 96/283 [09:02<03:26,  1.11s/it]\u001b[A\n",
            " 34%|███▍      | 97/283 [09:03<03:13,  1.04s/it]\u001b[A\n",
            " 35%|███▍      | 98/283 [09:03<03:05,  1.00s/it]\u001b[A\n",
            " 35%|███▍      | 99/283 [09:04<02:58,  1.03it/s]\u001b[A\n",
            " 35%|███▌      | 100/283 [09:05<02:57,  1.03it/s]\u001b[A\n",
            " 36%|███▌      | 101/283 [09:06<02:51,  1.06it/s]\u001b[A\n",
            " 36%|███▌      | 102/283 [09:07<02:46,  1.09it/s]\u001b[A\n",
            " 36%|███▋      | 103/283 [09:08<02:44,  1.10it/s]\u001b[A\n",
            " 37%|███▋      | 104/283 [09:09<02:45,  1.08it/s]\u001b[A\n",
            " 37%|███▋      | 105/283 [09:10<02:41,  1.10it/s]\u001b[A\n",
            " 37%|███▋      | 106/283 [09:11<02:38,  1.12it/s]\u001b[A\n",
            " 38%|███▊      | 107/283 [09:12<02:36,  1.12it/s]\u001b[A\n",
            " 38%|███▊      | 108/283 [09:12<02:34,  1.13it/s]\u001b[A\n",
            " 39%|███▊      | 109/283 [09:13<02:32,  1.14it/s]\u001b[A\n",
            " 39%|███▉      | 110/283 [09:14<02:30,  1.15it/s]\u001b[A\n",
            " 39%|███▉      | 111/283 [09:15<02:29,  1.15it/s]\u001b[A\n",
            " 40%|███▉      | 112/283 [09:16<02:27,  1.16it/s]\u001b[A\n",
            " 40%|███▉      | 113/283 [09:17<02:26,  1.16it/s]\u001b[A\n",
            " 40%|████      | 114/283 [09:18<02:25,  1.16it/s]\u001b[A\n",
            " 41%|████      | 115/283 [09:18<02:24,  1.16it/s]\u001b[A\n",
            " 41%|████      | 116/283 [09:19<02:24,  1.16it/s]\u001b[A\n",
            " 41%|████▏     | 117/283 [09:51<27:35,  9.98s/it]\u001b[A\n",
            " 42%|████▏     | 118/283 [09:52<20:12,  7.35s/it]\u001b[A\n",
            " 42%|████▏     | 119/283 [09:53<14:46,  5.40s/it]\u001b[A\n",
            " 42%|████▏     | 120/283 [09:53<10:59,  4.04s/it]\u001b[A\n",
            " 43%|████▎     | 121/283 [09:54<08:20,  3.09s/it]\u001b[A\n",
            " 43%|████▎     | 122/283 [09:55<06:30,  2.43s/it]\u001b[A\n",
            " 43%|████▎     | 123/283 [09:56<05:12,  1.96s/it]\u001b[A\n",
            " 44%|████▍     | 124/283 [09:57<04:18,  1.63s/it]\u001b[A\n",
            " 44%|████▍     | 125/283 [09:58<03:41,  1.40s/it]\u001b[A\n",
            " 45%|████▍     | 126/283 [09:59<03:14,  1.24s/it]\u001b[A\n",
            " 45%|████▍     | 127/283 [10:00<02:55,  1.12s/it]\u001b[A\n",
            " 45%|████▌     | 128/283 [10:00<02:42,  1.05s/it]\u001b[A\n",
            " 46%|████▌     | 129/283 [10:01<02:33,  1.00it/s]\u001b[A\n",
            " 46%|████▌     | 130/283 [10:02<02:26,  1.04it/s]\u001b[A\n",
            " 46%|████▋     | 131/283 [10:03<02:22,  1.07it/s]\u001b[A\n",
            " 47%|████▋     | 132/283 [10:04<02:18,  1.09it/s]\u001b[A\n",
            " 47%|████▋     | 133/283 [10:05<02:15,  1.11it/s]\u001b[A\n",
            " 47%|████▋     | 134/283 [10:06<02:12,  1.12it/s]\u001b[A\n",
            " 48%|████▊     | 135/283 [10:07<02:10,  1.14it/s]\u001b[A\n",
            " 48%|████▊     | 136/283 [10:07<02:08,  1.14it/s]\u001b[A\n",
            " 48%|████▊     | 137/283 [10:08<02:06,  1.15it/s]\u001b[A\n",
            " 49%|████▉     | 138/283 [10:09<02:05,  1.15it/s]\u001b[A\n",
            " 49%|████▉     | 139/283 [10:10<02:04,  1.15it/s]\u001b[A\n",
            " 49%|████▉     | 140/283 [10:11<02:04,  1.15it/s]\u001b[A\n",
            " 50%|████▉     | 141/283 [10:12<02:04,  1.14it/s]\u001b[A\n",
            " 50%|█████     | 142/283 [10:13<02:02,  1.15it/s]\u001b[A\n",
            " 51%|█████     | 143/283 [10:13<02:01,  1.15it/s]\u001b[A\n",
            " 51%|█████     | 144/283 [10:14<02:00,  1.15it/s]\u001b[A\n",
            " 51%|█████     | 145/283 [10:15<02:00,  1.14it/s]\u001b[A\n",
            " 52%|█████▏    | 146/283 [10:16<01:59,  1.15it/s]\u001b[A\n",
            " 52%|█████▏    | 147/283 [10:17<01:57,  1.15it/s]\u001b[A\n",
            " 52%|█████▏    | 148/283 [10:18<01:56,  1.16it/s]\u001b[A\n",
            " 53%|█████▎    | 149/283 [10:19<01:55,  1.16it/s]\u001b[A\n",
            " 53%|█████▎    | 150/283 [10:20<01:54,  1.16it/s]\u001b[A\n",
            " 53%|█████▎    | 151/283 [10:20<01:54,  1.16it/s]\u001b[A\n",
            " 54%|█████▎    | 152/283 [10:21<01:53,  1.16it/s]\u001b[A\n",
            " 54%|█████▍    | 153/283 [10:52<21:22,  9.86s/it]\u001b[A\n",
            " 54%|█████▍    | 154/283 [10:53<15:38,  7.28s/it]\u001b[A\n",
            " 55%|█████▍    | 155/283 [10:54<11:25,  5.35s/it]\u001b[A\n",
            " 55%|█████▌    | 156/283 [10:55<08:28,  4.00s/it]\u001b[A\n",
            " 55%|█████▌    | 157/283 [10:56<06:28,  3.09s/it]\u001b[A\n",
            " 56%|█████▌    | 158/283 [10:57<05:02,  2.42s/it]\u001b[A\n",
            " 56%|█████▌    | 159/283 [10:58<04:02,  1.95s/it]\u001b[A\n",
            " 57%|█████▋    | 160/283 [10:59<03:20,  1.63s/it]\u001b[A\n",
            " 57%|█████▋    | 161/283 [10:59<02:50,  1.40s/it]\u001b[A\n",
            " 57%|█████▋    | 162/283 [11:00<02:29,  1.24s/it]\u001b[A\n",
            " 58%|█████▊    | 163/283 [11:01<02:14,  1.12s/it]\u001b[A\n",
            " 58%|█████▊    | 164/283 [11:02<02:04,  1.05s/it]\u001b[A\n",
            " 58%|█████▊    | 165/283 [11:03<01:57,  1.00it/s]\u001b[A\n",
            " 59%|█████▊    | 166/283 [11:04<01:52,  1.04it/s]\u001b[A\n",
            " 59%|█████▉    | 167/283 [11:05<01:48,  1.07it/s]\u001b[A\n",
            " 59%|█████▉    | 168/283 [11:06<01:44,  1.10it/s]\u001b[A\n",
            " 60%|█████▉    | 169/283 [11:06<01:42,  1.12it/s]\u001b[A\n",
            " 60%|██████    | 170/283 [11:07<01:39,  1.13it/s]\u001b[A\n",
            " 60%|██████    | 171/283 [11:08<01:38,  1.14it/s]\u001b[A\n",
            " 61%|██████    | 172/283 [11:09<01:36,  1.15it/s]\u001b[A\n",
            " 61%|██████    | 173/283 [11:10<01:35,  1.15it/s]\u001b[A\n",
            " 61%|██████▏   | 174/283 [11:11<01:38,  1.10it/s]\u001b[A\n",
            " 62%|██████▏   | 175/283 [11:12<01:36,  1.12it/s]\u001b[A\n",
            " 62%|██████▏   | 176/283 [11:13<01:34,  1.13it/s]\u001b[A\n",
            " 63%|██████▎   | 177/283 [11:13<01:32,  1.14it/s]\u001b[A\n",
            " 63%|██████▎   | 178/283 [11:14<01:31,  1.14it/s]\u001b[A\n",
            " 63%|██████▎   | 179/283 [11:15<01:30,  1.15it/s]\u001b[A\n",
            " 64%|██████▎   | 180/283 [11:16<01:32,  1.12it/s]\u001b[A\n",
            " 64%|██████▍   | 181/283 [11:17<01:30,  1.13it/s]\u001b[A\n",
            " 64%|██████▍   | 182/283 [11:18<01:29,  1.13it/s]\u001b[A\n",
            " 65%|██████▍   | 183/283 [11:19<01:27,  1.14it/s]\u001b[A\n",
            " 65%|██████▌   | 184/283 [11:20<01:26,  1.14it/s]\u001b[A\n",
            " 65%|██████▌   | 185/283 [11:20<01:26,  1.13it/s]\u001b[A\n",
            " 66%|██████▌   | 186/283 [11:21<01:25,  1.14it/s]\u001b[A\n",
            " 66%|██████▌   | 187/283 [11:22<01:24,  1.14it/s]\u001b[A\n",
            " 66%|██████▋   | 188/283 [11:23<01:23,  1.13it/s]\u001b[A\n",
            " 67%|██████▋   | 189/283 [11:54<15:38,  9.98s/it]\u001b[A\n",
            " 67%|██████▋   | 190/283 [11:55<11:15,  7.26s/it]\u001b[A\n",
            " 67%|██████▋   | 191/283 [12:26<22:02, 14.37s/it]\u001b[A\n",
            " 68%|██████▊   | 192/283 [12:57<29:19, 19.33s/it]\u001b[A\n",
            " 68%|██████▊   | 193/283 [13:28<34:11, 22.80s/it]\u001b[A\n",
            " 69%|██████▊   | 194/283 [13:29<24:05, 16.24s/it]\u001b[A\n",
            " 69%|██████▉   | 195/283 [13:30<17:03, 11.63s/it]\u001b[A\n",
            " 69%|██████▉   | 196/283 [13:31<12:10,  8.40s/it]\u001b[A\n",
            " 70%|██████▉   | 197/283 [13:32<08:50,  6.17s/it]\u001b[A\n",
            " 70%|██████▉   | 198/283 [13:33<06:29,  4.58s/it]\u001b[A\n",
            " 70%|███████   | 199/283 [13:33<04:54,  3.50s/it]\u001b[A\n",
            " 71%|███████   | 200/283 [13:34<03:45,  2.72s/it]\u001b[A\n",
            " 71%|███████   | 201/283 [13:35<02:57,  2.16s/it]\u001b[A\n",
            " 71%|███████▏  | 202/283 [13:36<02:23,  1.77s/it]\u001b[A\n",
            " 72%|███████▏  | 203/283 [13:37<01:59,  1.50s/it]\u001b[A\n",
            " 72%|███████▏  | 204/283 [13:38<01:43,  1.31s/it]\u001b[A\n",
            " 72%|███████▏  | 205/283 [13:39<01:31,  1.18s/it]\u001b[A\n",
            " 73%|███████▎  | 206/283 [13:40<01:23,  1.09s/it]\u001b[A\n",
            " 73%|███████▎  | 207/283 [13:40<01:17,  1.02s/it]\u001b[A\n",
            " 73%|███████▎  | 208/283 [13:41<01:13,  1.02it/s]\u001b[A\n",
            " 74%|███████▍  | 209/283 [13:42<01:09,  1.06it/s]\u001b[A\n",
            " 74%|███████▍  | 210/283 [13:43<01:07,  1.08it/s]\u001b[A\n",
            " 75%|███████▍  | 211/283 [13:44<01:05,  1.10it/s]\u001b[A\n",
            " 75%|███████▍  | 212/283 [13:45<01:03,  1.12it/s]\u001b[A\n",
            " 75%|███████▌  | 213/283 [13:46<01:02,  1.13it/s]\u001b[A\n",
            " 76%|███████▌  | 214/283 [13:47<01:01,  1.12it/s]\u001b[A\n",
            " 76%|███████▌  | 215/283 [13:47<01:00,  1.12it/s]\u001b[A\n",
            " 76%|███████▋  | 216/283 [13:48<00:59,  1.13it/s]\u001b[A\n",
            " 77%|███████▋  | 217/283 [13:49<00:58,  1.14it/s]\u001b[A\n",
            " 77%|███████▋  | 218/283 [13:50<01:00,  1.07it/s]\u001b[A\n",
            " 77%|███████▋  | 219/283 [13:51<01:00,  1.06it/s]\u001b[A\n",
            " 78%|███████▊  | 220/283 [13:52<01:00,  1.04it/s]\u001b[A\n",
            " 78%|███████▊  | 221/283 [13:53<00:59,  1.04it/s]\u001b[A\n",
            " 78%|███████▊  | 222/283 [13:54<00:57,  1.07it/s]\u001b[A\n",
            " 79%|███████▉  | 223/283 [13:55<00:55,  1.09it/s]\u001b[A\n",
            " 79%|███████▉  | 224/283 [13:56<00:53,  1.11it/s]\u001b[A\n",
            " 80%|███████▉  | 225/283 [13:57<00:51,  1.12it/s]\u001b[A\n",
            " 80%|███████▉  | 226/283 [13:58<00:50,  1.13it/s]\u001b[A\n",
            " 80%|████████  | 227/283 [14:28<09:13,  9.89s/it]\u001b[A\n",
            " 81%|████████  | 228/283 [14:59<14:50, 16.20s/it]\u001b[A\n",
            " 81%|████████  | 229/283 [15:00<10:27, 11.63s/it]\u001b[A\n",
            " 81%|████████▏ | 230/283 [15:01<07:25,  8.40s/it]\u001b[A\n",
            " 82%|████████▏ | 231/283 [15:02<05:19,  6.14s/it]\u001b[A\n",
            " 82%|████████▏ | 232/283 [15:03<03:52,  4.57s/it]\u001b[A\n",
            " 82%|████████▏ | 233/283 [15:04<02:52,  3.46s/it]\u001b[A\n",
            " 83%|████████▎ | 234/283 [15:05<02:11,  2.68s/it]\u001b[A\n",
            " 83%|████████▎ | 235/283 [15:06<01:42,  2.14s/it]\u001b[A\n",
            " 83%|████████▎ | 236/283 [15:06<01:22,  1.75s/it]\u001b[A\n",
            " 84%|████████▎ | 237/283 [15:07<01:08,  1.49s/it]\u001b[A\n",
            " 84%|████████▍ | 238/283 [15:08<00:58,  1.30s/it]\u001b[A\n",
            " 84%|████████▍ | 239/283 [15:09<00:51,  1.17s/it]\u001b[A\n",
            " 85%|████████▍ | 240/283 [15:10<00:46,  1.08s/it]\u001b[A\n",
            " 85%|████████▌ | 241/283 [15:11<00:42,  1.02s/it]\u001b[A\n",
            " 86%|████████▌ | 242/283 [15:12<00:39,  1.03it/s]\u001b[A\n",
            " 86%|████████▌ | 243/283 [15:13<00:37,  1.05it/s]\u001b[A\n",
            " 86%|████████▌ | 244/283 [15:14<00:37,  1.04it/s]\u001b[A\n",
            " 87%|████████▋ | 245/283 [15:14<00:35,  1.07it/s]\u001b[A\n",
            " 87%|████████▋ | 246/283 [15:15<00:33,  1.09it/s]\u001b[A\n",
            " 87%|████████▋ | 247/283 [15:16<00:32,  1.11it/s]\u001b[A\n",
            " 88%|████████▊ | 248/283 [15:17<00:31,  1.11it/s]\u001b[A\n",
            " 88%|████████▊ | 249/283 [15:18<00:30,  1.13it/s]\u001b[A\n",
            " 88%|████████▊ | 250/283 [15:19<00:28,  1.14it/s]\u001b[A\n",
            " 89%|████████▊ | 251/283 [15:20<00:27,  1.14it/s]\u001b[A\n",
            " 89%|████████▉ | 252/283 [15:20<00:26,  1.15it/s]\u001b[A\n",
            " 89%|████████▉ | 253/283 [15:21<00:26,  1.15it/s]\u001b[A\n",
            " 90%|████████▉ | 254/283 [15:22<00:25,  1.15it/s]\u001b[A\n",
            " 90%|█████████ | 255/283 [15:23<00:24,  1.14it/s]\u001b[A\n",
            " 90%|█████████ | 256/283 [15:24<00:23,  1.14it/s]\u001b[A\n",
            " 91%|█████████ | 257/283 [15:25<00:22,  1.14it/s]\u001b[A\n",
            " 91%|█████████ | 258/283 [15:26<00:21,  1.15it/s]\u001b[A\n",
            " 92%|█████████▏| 259/283 [15:27<00:20,  1.15it/s]\u001b[A\n",
            " 92%|█████████▏| 260/283 [15:27<00:20,  1.15it/s]\u001b[A\n",
            " 92%|█████████▏| 261/283 [15:29<00:26,  1.19s/it]\u001b[A\n",
            " 93%|█████████▎| 262/283 [16:00<03:32, 10.10s/it]\u001b[A\n",
            " 93%|█████████▎| 263/283 [16:31<05:27, 16.36s/it]\u001b[A\n",
            " 93%|█████████▎| 264/283 [16:32<03:42, 11.71s/it]\u001b[A\n",
            " 94%|█████████▎| 265/283 [17:03<05:14, 17.46s/it]\u001b[A\n",
            " 94%|█████████▍| 266/283 [17:34<06:05, 21.49s/it]\u001b[A\n",
            " 94%|█████████▍| 267/283 [17:35<04:06, 15.41s/it]\u001b[A\n",
            " 95%|█████████▍| 268/283 [17:36<02:45, 11.05s/it]\u001b[A\n",
            " 95%|█████████▌| 269/283 [17:37<01:52,  8.01s/it]\u001b[A\n",
            " 95%|█████████▌| 270/283 [17:38<01:16,  5.87s/it]\u001b[A\n",
            " 96%|█████████▌| 271/283 [17:39<00:52,  4.37s/it]\u001b[A\n",
            " 96%|█████████▌| 272/283 [17:40<00:36,  3.32s/it]\u001b[A\n",
            " 96%|█████████▋| 273/283 [17:40<00:25,  2.59s/it]\u001b[A\n",
            " 97%|█████████▋| 274/283 [17:41<00:18,  2.08s/it]\u001b[A\n",
            " 97%|█████████▋| 275/283 [17:42<00:13,  1.72s/it]\u001b[A\n",
            " 98%|█████████▊| 276/283 [17:43<00:10,  1.47s/it]\u001b[A\n",
            " 98%|█████████▊| 277/283 [17:44<00:07,  1.31s/it]\u001b[A\n",
            " 98%|█████████▊| 278/283 [17:45<00:05,  1.18s/it]\u001b[A\n",
            " 99%|█████████▊| 279/283 [17:46<00:04,  1.11s/it]\u001b[A\n",
            " 99%|█████████▉| 280/283 [17:47<00:03,  1.07s/it]\u001b[A\n",
            " 99%|█████████▉| 281/283 [17:48<00:02,  1.02s/it]\u001b[A\n",
            "100%|█████████▉| 282/283 [17:49<00:00,  1.03it/s]\u001b[A\n",
            "100%|██████████| 283/283 [17:49<00:00,  3.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded GRD files: 282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 282/282 [00:00<00:00, 2034.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved combined CSV: /content/imdpune_max/district_daily_max_temp_2025.csv\n",
            "CSV shape: (282, 729)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dc2f9830-f57a-45ff-abb0-ef53429e8b4a\", \"district_daily_max_temp_2025.csv\", 3784201)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'> <font size ='5'> **Realtime Districts Daily rainfall data download for grid data**"
      ],
      "metadata": {
        "id": "jLS50WJOWgV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Colab-ready: Download IMD Daily Rainfall ----------------\n",
        "!pip install -q geopandas tqdm shapely fiona bs4 requests\n",
        "\n",
        "import os, requests, zipfile, glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from tqdm import tqdm\n",
        "from shapely.geometry import Point\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "IMD_PAGE = \"https://imdpune.gov.in/lrfindex.php\"\n",
        "GRD_BASE = \"https://imdpune.gov.in/cmpg/Griddata/Rainfall/\"\n",
        "WORKDIR = \"/content/imdpune_rain\"\n",
        "GRD_DIR = os.path.join(WORKDIR, \"grds\")\n",
        "GEO_DIR = os.path.join(WORKDIR, \"geoboundaries\")\n",
        "OUT_CSV = os.path.join(WORKDIR, \"district_daily_rain.csv\")\n",
        "os.makedirs(GRD_DIR, exist_ok=True)\n",
        "os.makedirs(GEO_DIR, exist_ok=True)\n",
        "\n",
        "# IMD 0.25° grid\n",
        "ncols, nrows = 135, 129\n",
        "x_start, y_start = 66.5, 6.5\n",
        "x_step, y_step = 0.25, 0.25\n",
        "nodata_value = -999.0\n",
        "\n",
        "# ---------------- 1) Download district shapefile ----------------\n",
        "GEOBOUNDARIES_IND_ADM2_ZIP = \"https://github.com/wmgeolab/geoBoundaries/raw/9469f09/releaseData/gbOpen/IND/ADM2/geoBoundaries-IND-ADM2-all.zip\"\n",
        "geo_zip_local = os.path.join(GEO_DIR, \"geoBoundaries-IND-ADM2-all.zip\")\n",
        "\n",
        "if not os.path.exists(geo_zip_local):\n",
        "    print(\"Downloading district boundaries...\")\n",
        "    r = requests.get(GEOBOUNDARIES_IND_ADM2_ZIP, stream=True)\n",
        "    if r.status_code == 200:\n",
        "        with open(geo_zip_local, \"wb\") as f:\n",
        "            for chunk in tqdm(r.iter_content(chunk_size=8192),\n",
        "                            desc='Downloading boundaries'):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "# Extract and find shapefile\n",
        "shp_file = None\n",
        "if os.path.exists(geo_zip_local):\n",
        "    with zipfile.ZipFile(geo_zip_local, \"r\") as z:\n",
        "        z.extractall(GEO_DIR)\n",
        "\n",
        "    for root, dirs, files in os.walk(GEO_DIR):\n",
        "        for file in files:\n",
        "            if file.endswith('.shp'):\n",
        "                shp_file = os.path.join(root, file)\n",
        "                break\n",
        "        if shp_file:\n",
        "            break\n",
        "\n",
        "if shp_file and os.path.exists(shp_file):\n",
        "    gdf_admin = gpd.read_file(shp_file)\n",
        "    gdf_admin_proj = gdf_admin.to_crs(epsg=3857)\n",
        "    gdf_admin_proj[\"centroid\"] = gdf_admin_proj.geometry.centroid\n",
        "    gdf_admin_proj = gdf_admin_proj.to_crs(epsg=4326)\n",
        "    gdf_admin[\"centroid\"] = gdf_admin_proj[\"centroid\"]\n",
        "\n",
        "    district_name_field = next((f for f in [\"shapeName\", \"shapeName_en\", \"NAME_2\", \"district\", \"District\", \"NAME_1\", \"NAME\"]\n",
        "                              if f in gdf_admin.columns), gdf_admin.columns[0])\n",
        "    print(f\"Using district name field: {district_name_field}\")\n",
        "    print(f\"Total districts: {len(gdf_admin)}\")\n",
        "else:\n",
        "    print(\"Error: Could not find or load shapefile\")\n",
        "    exit()\n",
        "\n",
        "# ---------------- 2) DOWNLOAD DAILY DATA FROM 1st JAN 2025 TO LATEST ----------------\n",
        "print(\"Downloading daily rainfall data from 1st January 2025...\")\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "def download_file(year, month, day):\n",
        "    \"\"\"Download specific GRD file\"\"\"\n",
        "    yy = str(year)[2:].zfill(2)\n",
        "    mm = str(month).zfill(2)\n",
        "    dd = str(day).zfill(2)\n",
        "\n",
        "    filename = f\"rain_ind0.25_{yy}_{mm}_{dd}.grd\"\n",
        "    url = f\"{GRD_BASE}{filename}\"\n",
        "    local_path = os.path.join(GRD_DIR, filename)\n",
        "\n",
        "    # Skip if already exists\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        if response.status_code == 200:\n",
        "            # Check if it's a valid binary file\n",
        "            if len(response.content) > 1000 and b'html' not in response.content[:100].lower():\n",
        "                with open(local_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                return local_path\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return None\n",
        "\n",
        "# Generate all dates from 1st Jan 2025 to today\n",
        "start_date = datetime(2025, 1, 1)\n",
        "end_date = datetime.now()\n",
        "current_date = start_date\n",
        "\n",
        "print(f\"Checking dates from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "downloaded_files = []\n",
        "total_days = (end_date - start_date).days + 1\n",
        "\n",
        "# Test and download files\n",
        "for day_offset in tqdm(range(total_days), desc=\"Checking dates\"):\n",
        "    current_date = start_date + timedelta(days=day_offset)\n",
        "    file_path = download_file(current_date.year, current_date.month, current_date.day)\n",
        "    if file_path:\n",
        "        downloaded_files.append(file_path)\n",
        "\n",
        "print(f\"Downloaded {len(downloaded_files)} daily rainfall files\")\n",
        "\n",
        "# ---------------- 3) Map grid to districts ----------------\n",
        "print(\"Mapping districts to grid cells...\")\n",
        "gdf_admin[\"nearest_r\"] = ((gdf_admin.centroid.y - y_start) / y_step).round().astype(int)\n",
        "gdf_admin[\"nearest_c\"] = ((gdf_admin.centroid.x - x_start) / x_step).round().astype(int)\n",
        "\n",
        "# Filter valid grid indices\n",
        "valid_mask = (gdf_admin[\"nearest_r\"] >= 0) & (gdf_admin[\"nearest_r\"] < nrows) & \\\n",
        "             (gdf_admin[\"nearest_c\"] >= 0) & (gdf_admin[\"nearest_c\"] < ncols)\n",
        "\n",
        "gdf_admin_valid = gdf_admin[valid_mask]\n",
        "district_to_grid = dict(zip(gdf_admin_valid[district_name_field],\n",
        "                           zip(gdf_admin_valid[\"nearest_r\"], gdf_admin_valid[\"nearest_c\"])))\n",
        "\n",
        "print(f\"Districts mapped: {len(district_to_grid)}\")\n",
        "\n",
        "# ---------------- 4) Read .GRD file ----------------\n",
        "def read_grid(path):\n",
        "    try:\n",
        "        arr = np.fromfile(path, dtype=np.float32)\n",
        "        expected_size = nrows * ncols\n",
        "\n",
        "        if arr.size != expected_size:\n",
        "            if arr.size > expected_size:\n",
        "                arr = arr[:expected_size]\n",
        "            else:\n",
        "                full_arr = np.full(expected_size, nodata_value, dtype=np.float32)\n",
        "                full_arr[:arr.size] = arr\n",
        "                arr = full_arr\n",
        "\n",
        "        return arr.reshape((nrows, ncols))\n",
        "    except:\n",
        "        return np.full((nrows, ncols), nodata_value, dtype=np.float32)\n",
        "\n",
        "# ---------------- 5) Process files and create CSV ----------------\n",
        "print(\"Processing daily rainfall data...\")\n",
        "records = []\n",
        "\n",
        "# Sort files by date\n",
        "def extract_date(path):\n",
        "    filename = os.path.basename(path)\n",
        "    pattern = r'rain_ind0\\.25_(\\d+)_(\\d+)_(\\d+)\\.grd'\n",
        "    match = re.search(pattern, filename)\n",
        "    if match:\n",
        "        yy, mm, dd = match.groups()\n",
        "        return datetime(2000 + int(yy), int(mm), int(dd))\n",
        "    return datetime(1900, 1, 1)\n",
        "\n",
        "if downloaded_files:\n",
        "    downloaded_files.sort(key=extract_date)\n",
        "\n",
        "    for path in tqdm(downloaded_files, desc=\"Processing files\"):\n",
        "        try:\n",
        "            arr = read_grid(path)\n",
        "            filename = os.path.basename(path)\n",
        "\n",
        "            pattern = r'rain_ind0\\.25_(\\d+)_(\\d+)_(\\d+)\\.grd'\n",
        "            match = re.search(pattern, filename)\n",
        "\n",
        "            if match:\n",
        "                yy, mm, dd = match.groups()\n",
        "                year = 2000 + int(yy)\n",
        "                date_str = datetime(year, int(mm), int(dd)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "                record = {\"date\": date_str}\n",
        "\n",
        "                for district, (r, c) in district_to_grid.items():\n",
        "                    val = arr[r, c]\n",
        "                    if abs(val - nodata_value) < 0.001 or val < 0:\n",
        "                        record[district] = float('nan')\n",
        "                    else:\n",
        "                        record[district] = float(val)\n",
        "\n",
        "                records.append(record)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "# ---------------- 6) Save clean CSV ----------------\n",
        "if records:\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Reorder columns: date first, then alphabetical districts\n",
        "    cols = [\"date\"] + sorted([c for c in df.columns if c != \"date\"])\n",
        "    df = df[cols].sort_values(\"date\").reset_index(drop=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv(OUT_CSV, index=False)\n",
        "\n",
        "    print(f\"✅ SUCCESS: Daily rainfall data saved\")\n",
        "    print(f\"📅 Dates: {len(records)} days\")\n",
        "    print(f\"🏛️  Districts: {len(district_to_grid)}\")\n",
        "    print(f\"📁 File: {OUT_CSV}\")\n",
        "\n",
        "    # Show simple sample\n",
        "    print(f\"\\nSample data:\")\n",
        "    print(f\"First date: {df['date'].min()}\")\n",
        "    print(f\"Last date: {df['date'].max()}\")\n",
        "    print(f\"Total records: {len(df)}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ No data processed\")\n",
        "\n",
        "# ---------------- 7) Download CSV ----------------\n",
        "try:\n",
        "    from google.colab import files\n",
        "    if os.path.exists(OUT_CSV):\n",
        "        files.download(OUT_CSV)\n",
        "        print(f\"📥 File downloaded: {OUT_CSV}\")\n",
        "except:\n",
        "    print(f\"💾 File saved: {OUT_CSV}\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "OJERcOFZY5RI",
        "outputId": "2295ad9d-97fc-411c-9e5d-8dd9f872cc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using district name field: shapeName\n",
            "Total districts: 735\n",
            "Downloading daily rainfall data from 1st January 2025...\n",
            "Checking dates from 2025-01-01 to 2025-10-15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checking dates: 100%|██████████| 288/288 [04:26<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 4 daily rainfall files\n",
            "Mapping districts to grid cells...\n",
            "Districts mapped: 728\n",
            "Processing daily rainfall data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing files: 100%|██████████| 4/4 [00:00<00:00, 640.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SUCCESS: Daily rainfall data saved\n",
            "📅 Dates: 4 days\n",
            "🏛️  Districts: 728\n",
            "📁 File: /content/imdpune_rain/district_daily_rain.csv\n",
            "\n",
            "Sample data:\n",
            "First date: 2025-01-15\n",
            "Last date: 2025-10-01\n",
            "Total records: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_503d5f96-bd47-44b1-b2c3-96a41839a351\", \"district_daily_rain.csv\", 60655)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 File downloaded: /content/imdpune_rain/district_daily_rain.csv\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='green'> <font size ='5'> **Real-time Districts Daily raifall data download for grid data**"
      ],
      "metadata": {
        "id": "PEr6tAl0WFkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================\n",
        "#      IMD RAINFALL SCRIPT — WITH CTL UPLOAD\n",
        "# ================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import geopandas as gpd\n",
        "from tqdm import tqdm\n",
        "from shapely.geometry import Point\n",
        "from google.colab import files\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1) USER UPLOADS CTL FILE\n",
        "# ------------------------------------------------\n",
        "print(\"📌 PLEASE UPLOAD YOUR CTL FILE (e.g., ddmmyyyy.ctl)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "CTL_PATH = list(uploaded.keys())[0]\n",
        "print(\"CTL file received:\", CTL_PATH)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2) PARSE CTL METADATA\n",
        "# ------------------------------------------------\n",
        "meta = {}\n",
        "with open(CTL_PATH, \"r\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        k = parts[0].upper()\n",
        "\n",
        "        if k == \"UNDEF\":\n",
        "            meta[\"undef\"] = float(parts[1])\n",
        "        elif k == \"XDEF\":\n",
        "            meta[\"ncols\"] = int(parts[1])\n",
        "            meta[\"x_start\"] = float(parts[3])\n",
        "            meta[\"x_step\"] = float(parts[4])\n",
        "        elif k == \"YDEF\":\n",
        "            meta[\"nrows\"] = int(parts[1])\n",
        "            meta[\"y_start\"] = float(parts[3])\n",
        "            meta[\"y_step\"] = float(parts[4])\n",
        "\n",
        "print(\"Parsed CTL:\", meta)\n",
        "\n",
        "ncols = meta[\"ncols\"]\n",
        "nrows = meta[\"nrows\"]\n",
        "x_start = meta[\"x_start\"]\n",
        "y_start = meta[\"y_start\"]\n",
        "x_step = meta[\"x_step\"]\n",
        "y_step = meta[\"y_step\"]\n",
        "undef = meta[\"undef\"]\n",
        "\n",
        "print(\"\\nGRID INFO:\")\n",
        "print(\"Rows:\", nrows, \"Cols:\", ncols)\n",
        "print(\"X start:\", x_start, \"Step:\", x_step)\n",
        "print(\"Y start:\", y_start, \"Step:\", y_step)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3) DOWNLOAD INDIA DISTRICT SHP\n",
        "# ------------------------------------------------\n",
        "GEO_URL = \"https://github.com/wmgeolab/geoBoundaries/raw/9469f09/releaseData/gbOpen/IND/ADM2/geoBoundaries-IND-ADM2-all.zip\"\n",
        "ZIP_PATH = \"/content/adm2.zip\"\n",
        "\n",
        "if not os.path.exists(ZIP_PATH):\n",
        "    print(\"Downloading district boundaries...\")\n",
        "    r = requests.get(GEO_URL)\n",
        "    open(ZIP_PATH, \"wb\").write(r.content)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "    z.extractall(\"/content/adm2/\")\n",
        "\n",
        "geo_file = \"\"\n",
        "for f in os.listdir(\"/content/adm2/\"):\n",
        "    if f.lower().endswith(\".geojson\"):\n",
        "        geo_file = \"/content/adm2/\" + f\n",
        "        break\n",
        "\n",
        "gdf = gpd.read_file(geo_file).to_crs(epsg=4326)\n",
        "\n",
        "# district name field\n",
        "for col in [\"shapeName\", \"NAME_2\", \"NAME\", \"district\"]:\n",
        "    if col in gdf.columns:\n",
        "        district_field = col\n",
        "        break\n",
        "\n",
        "print(\"District name field:\", district_field)\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 4) DISTRICT → GRID MAPPING (centroid)\n",
        "# ------------------------------------------------\n",
        "gdf_proj = gdf.to_crs(3857)\n",
        "gdf[\"centroid\"] = gdf_proj.centroid.to_crs(4326)\n",
        "\n",
        "gdf[\"row\"] = ((gdf.centroid.y - y_start) / y_step).round().astype(int)\n",
        "gdf[\"col\"] = ((gdf.centroid.x - x_start) / x_step).round().astype(int)\n",
        "\n",
        "gdf[\"row\"] = gdf[\"row\"].clip(0, nrows - 1)\n",
        "gdf[\"col\"] = gdf[\"col\"].clip(0, ncols - 1)\n",
        "\n",
        "district_map = dict(zip(gdf[district_field], zip(gdf[\"row\"], gdf[\"col\"])))\n",
        "\n",
        "print(\"Mapped districts:\", len(district_map))\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 5) AUTO-DOWNLOAD ALL GRD FOR YEAR (2025)\n",
        "# ------------------------------------------------\n",
        "RAIN_BASE = \"https://www.imdpune.gov.in/cmpg/Realtimedata/Rainfall/\"\n",
        "START = datetime(2025, 1, 1)\n",
        "END   = datetime(2025, 12, 31)\n",
        "\n",
        "grd_dir = \"/content/grd/\"\n",
        "os.makedirs(grd_dir, exist_ok=True)\n",
        "\n",
        "def make_filename(d):\n",
        "    yy = d.year % 100\n",
        "    return f\"rain_ind0.25_{yy:02d}_{d.month:02d}_{d.day:02d}.grd\"\n",
        "\n",
        "downloaded = []\n",
        "d = START\n",
        "pbar = tqdm(total=(END - START).days + 1, desc=\"Downloading GRD\")\n",
        "\n",
        "while d <= END:\n",
        "    fname = make_filename(d)\n",
        "    url = RAIN_BASE + fname\n",
        "    local = grd_dir + fname\n",
        "\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "        open(local, \"wb\").write(r.content)\n",
        "        downloaded.append(local)\n",
        "    else:\n",
        "        print(\"Missing:\", fname)\n",
        "\n",
        "    d += timedelta(days=1)\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()\n",
        "print(\"Total GRD downloaded:\", len(downloaded))\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 6) READ GRD FILE\n",
        "# ------------------------------------------------\n",
        "def read_grd(path):\n",
        "    try:\n",
        "        return np.fromfile(path, dtype=\"<f4\").reshape((nrows, ncols))\n",
        "    except:\n",
        "        return np.fromfile(path, dtype=\">f4\").reshape((nrows, ncols))\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 7) PROCESS ALL DOWNLOADED GRD\n",
        "# ------------------------------------------------\n",
        "rows = []\n",
        "\n",
        "for path in tqdm(downloaded, desc=\"Processing GRDs\"):\n",
        "    fname = os.path.basename(path).replace(\".grd\", \"\")\n",
        "    parts = fname.split(\"_\")\n",
        "    # ['rain', 'ind0.25', '25', '07', '01']\n",
        "\n",
        "    yy = int(parts[2])\n",
        "    mm = int(parts[3])\n",
        "    dd = int(parts[4])\n",
        "    yyyy = 2000 + yy\n",
        "    date_str = f\"{yyyy:04d}-{mm:02d}-{dd:02d}\"\n",
        "\n",
        "    arr = read_grd(path)\n",
        "    rec = {\"date\": date_str}\n",
        "\n",
        "    for dist, (r, c) in district_map.items():\n",
        "        v = arr[r, c]\n",
        "        rec[dist] = np.nan if v == undef else float(v)\n",
        "\n",
        "    rows.append(rec)\n",
        "\n",
        "df = pd.DataFrame(rows).sort_values(\"date\")\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 8) SAVE CSV\n",
        "# ------------------------------------------------\n",
        "OUT = \"/content/IMD_Rainfall_Districtwise_2025.csv\"\n",
        "df.to_csv(OUT, index=False)\n",
        "\n",
        "print(\"\\n🎉 DONE! Your final CSV is ready:\")\n",
        "print(OUT)\n",
        "files.download(OUT)\n"
      ],
      "metadata": {
        "id": "ZOyrMKPPWF6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}